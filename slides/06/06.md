title: NPFL129, Lecture 6
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }

# Representing Text (TF-IDF, Word2vec)

## Jindřich Libovický <small>(reusing materials by Milan Straka)</small>

### November 7, 2023

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Use TF-IDF for representing documents and explain its information-theoretical
  interpretation.

- Explain training of Word2Vec as a special case of logistic regression.

- Use pre-trained word embeddings for simple NLP tasks.

---
section: EvalNLP
# Metrics for Exemplary Tasks

- **Part-of-speech tagging**: assign a part-of-speech to every word in the input
  text.

~~~
  - **accuracy** on such a task is the same as micro-averaged precision, recall, and $F_1$-score,
    because exactly one class is predicted for every word (i.e., TP+FP = TP+FN).

~~~
- **Named entity recognition**: recognize personal names, organizations, and
  locations in the input text.

~~~
  - **accuracy** is artificially high, because many words are not a named entity;
~~~
  - **micro-averaged** $F_1$ considers all named entities, with classes used
    only to decide if a prediction is correct;
    _“how good are we at recognizing all present named entities”_;
~~~
  - **macro-averaged** $F_1$
    _“how good are we at recognizing all named entities **types**”_.

~~~
Consider **multi-label classification**, where you can generate any number of
classes for an input example (while in the multiclass classification you generate
always exactly one).

~~~
- For example **text classification**: choose domains (sports/politics/…) for input documents.
~~~
- Can be solved analogously to softmax classification, only using sigmoid
  activation.
~~~
- Accuracy is very strict (all predicted classes must be exactly the same).
~~~
- Commonly evaluated using micro-averaged or macro-averaged $F_1$-score.

---
section: TF-IDF
# Document Representation

We already know how to represent images and categorical variables (classes,
letters, words, …).

~~~
Now consider the problem of representing a whole _document_.

~~~
An elementary approach is to represent a document as a **bag of words** – we
create a feature space with a dimension for every unique word (or for character
sequences), called a **term**.

~~~
However, there are many ways in which the values of the terms can be set.

---
# Term Frequency – Inverse Document Frequency

Commonly used ways of setting the term values:

- **binary indicators**: 1/0 depending on whether a term is present in
  a document or not;

~~~
- **term frequency (TF)**: relative frequency of a term in a document;
  $$\mathit{TF}(t; d) = \frac{\textrm{number of occurrences of $t$ in the document $d$}}{\textrm{number of terms in the document $d$}}$$
~~~
- **inverse document frequency (IDF)**: we could also represent a term using
  self-information of a probability of a random document containing it (therefore,
  terms with lower document probability have higher weights);
  $$\mathit{IDF}(t)
    = \log \frac{\textrm{number of documents}}{\textrm{number of documents containing $t$ }\big(\textrm{optionally} + 1)}
    = I\big(P(d ∋ t)\big)
  $$
~~~
- **TF-IDF**: empirically, product $\mathit{TF} ⋅ \mathit{IDF}$ is a feature
  reflecting quite well how important a term is to a document in a corpus
  (used by 83\% text-based recommender systems in 2015).

---
style: middle
# Original Motivation

![w=60%,f=left](zipf.png)

Jones (1972) provided only intuitive justification.

**Zipf's law**: empirically, word frequencies follow

$$\text{word frequency} \propto \frac{1}{\text{word rank}}$$

I.e., $\frac{\normalsize |𝓓|}{\normalsize |\{d∈𝓓:t∈d\}|}$ would be extremely
low for frequent words, and high for infrequent ones. Logarithm normalizes that.

---
section: MutualInformation
# Mutual Information

Consider two random variables $⁇x$ and $⁇y$ with distributions $⁇x ∼ X$ and $⁇y ∼ Y$.
~~~

The conditional entropy $H(Y | X)$ can be naturally considered an expectation of
a self-information of $Y | X$, so in the discrete case,
~~~
$$H(Y | X) = 𝔼_{⁇x,⁇y} \big[I(y|x)\big] = -∑_{x,y} P(x, y) \log P(y | x).$$

~~~
In order to assess the amount of information _shared_ between the two
random variables, we might consider the difference
$$\textcolor{red}{H(Y)} - \textcolor{blue}{H(Y | X)}
  = 𝔼_{⁇x,⁇y} \big[\textcolor{red}{-\log P(y)}\big] - 𝔼_{⁇x,⁇y} \big[\textcolor{blue}{-\log P(y|x)}\big]
  = 𝔼_{⁇x,⁇y} \left[\log\frac{\textcolor{blue}{P(x, y)}}{\textcolor{blue}{P(x)} \textcolor{red}{P(y)}}\right].$$

~~~
We can interpret this value as

> _How many bits of information will we learn about $Y$ when we find out $X$?_

---
# Mutual Information

![w=27%,f=right](mutual_information.svgz)

Let us denote this quantity as the **mutual information** $I(X; Y)$:
$$I(X; Y) = 𝔼_{⁇x,⁇y} \left[\log\frac{P(x, y)}{P(x)P(y)}\right].$$

~~~
- The mutual information is symmetrical, so
  $$I(X; Y) = I(Y; X) = H(Y) - H(Y|X) = H(X) - H(X | Y).$$

~~~
- It is easy to verify that
  $$I(X; Y) = D_\textrm{KL}\big(P(X, Y) \| P(X) P(Y)\big).$$

~~~
  Therefore,
  - $I(X; Y) ≥ 0$,
  - $I(X; Y) = 0$ iff $P(X, Y) = P(X) P(Y)$ iff the random variables are
    independent.

---
section: TF-IDF
# TF-IDF as Mutual Information

Let $𝓓$ be a collection of documents and $𝓣$ a collection of terms.

~~~
We assume that whenever we need to draw a document, we do it
uniformly randomly. Then,
~~~
- $P(d) = 1/|𝓓|$ and $I(d) = H(𝓓) = \log |𝓓|$,

~~~
- $P(d|t∈d) = 1/|\{d∈𝓓:t∈d\}|$,
~~~
- $I(d|t∈d) = H(𝓓|t) = \log |\{d∈𝓓:t∈d\}|$, assuming $0 ⋅ \log 0 = 0$ in $H$ as usual,
~~~
- $I(d) - I(d|t∈d) = H(𝓓) - H(𝓓|t) = \log \frac{\normalsize |𝓓|}{\normalsize |\{d∈𝓓:t∈d\}|} = \mathit{IDF}(t).$

~~~
Finally, we can compute the mutual information $I(𝓓; 𝓣)$ as
$$I(𝓓; 𝓣)
  = ∑_{d,\,t∈d} \textcolor{red}{P(d)} ⋅ \textcolor{blue}{P(t|d)} ⋅ \textcolor{darkgreen}{\big(I(d) - I(d|t)\big)}
  = \textcolor{red}{\frac{1}{|𝓓|}} ∑_{d,\,t∈d} \textcolor{blue}{\mathit{TF}(t;d)} ⋅ \textcolor{darkgreen}{\mathit{IDF(t)}}.
$$

~~~
Therefore, summing all TF-IDF terms recovers the mutual information between $𝓓$
and $𝓣$, and we can say that each TF-IDF carries a “bit of information” attached
to a document-term pair.

---
section: Word2Vec
class: middle
# Representation Learning

- We interpreted MLP as automatic feature extraction for a generalized linear
  model.
~~~

- Representation learning: learning using a proxy task that leads to reusable
  features.

~~~

\
\
\
Famous examples: pre-training image representations using **object
classification**, pre-training using **language models**.


---
class: middle
# Word Embeddings

- Assuming a limited vocabulary: an input word can be represented as a one-hot
  vector

~~~

- Multiplying a matrix with a one-hot vector = picking a vector from the weight
  matrix

~~~

- This matrix column = <big><b>word embedding</b><big>


---
# The oldest neural language model

- Language model predicts a probability of the next token – it used to be a
  component of machine translation and speech recognition.

~~~

- In 2003: Bengio et al. used MLP for language modeling

![w=35%,h=center](bengio_architecture.svgz)

- The embedding matrix is reused for all input words

---
# Properties of Word Embeddings

- Collobert et al. (2011) first reused embeddings from language modelling as
  features for other NLP tasks

- Geometric properties: Neighbors in the space are semantically similar words

![w=55%,h=center](collobert.svgz)

- In a downstream task, we learn something also for words that **were not in
  training data** but might similar to some that were.

---
# Word2Vec

<center><big>How to get vectors for as many words as possible without training a huge model?</big></center>

~~~

1. Simplify the context – treat it as a bag of words

2. Simplify the architecture – turn it to a linear model

~~~

![w=50%,h=center](cbow_and_skipgram.svgz)

---
class: middle
# Word2Vec: Skip-gram Sampling

![w=100%,h=center](skipgram_sampling.svgz)

---
# SkipGram: Idea of the Computation

- For each word from vocabulary $V$, we want to learn a $d$-dimension embedding vector $\boldsymbol{e}$

- For an embedding vector $\boldsymbol{e} \in \mathbb{R}^d$, we predict probability distribution
  of words that may appear in the context:
  $\operatorname{softmax}\left(\boldsymbol{e}^T \boldsymbol{W} \right)$

- $\boldsymbol{W} \in \mathbb{d \times |V|}$ is a parameter matrix shared for all embeddings $\boldsymbol{e}$

- Vocabularies of $~10^5-10^6$ word forms
  $\Rightarrow$ computing the $\operatorname{softmax}$ would be expensive.

- Turn it into **multi-label classification** and use negative sampling for
  loss function – we say each word independently if it appears in the word
  context.
\
\
\
$\sigma\left( EW \right)_{I,J}$ should estimate a table of $|V| \times |V|$
with probabilities that $j$-th word is in the neighbor window of $i$-th word

---
# SkipGram: Negative Sampling

 - Split the output matrix $\boldsymbol{W}$ into output embeddings $\boldsymbol{v}$

~~~
 - For input word $w$ with embeddings $\boldsymbol{e}_w$ and context word $c$
   with output embedding $\boldsymbol{v}_c$, we do logistic regression and the
   following to the loss function:

     $$-\log \sigma (\boldsymbol{e}_w^T\boldsymbol{v}_c)$$

~~~

 - Then we sample negative $K$ word sample $c_i$ that are **not** in the
   context window and add them negatively to the loss function:

     $$- \sum_{i=1}^K \log \sigma ( -\boldsymbol{e}_w^T \boldsymbol{v}_{c_i} )$$

~~~

  - The distribution we sample from is heuristically modified categorical
    distribution based on word frequencies

---
class: middle,center
# Interesting Properties of Word2Vec

It seems that vector arithmetics captures nicely lexical semantics.

<br />

![w=80%,h=center](w2v_relations.svgz)


---
# Using Word Embeddings in Our Models

- Single label per text:

  - Problem: text of variable length $\rightarrow$ most frequent solution

  - Just compute the average over the sequence (and think what should go into the average)

~~~

- Sequence labeling = assign a label per token

  - Many NLP tasks can be formulated as sequence labeling (POS tagging, named
    entity recognition, extractive summarization or QA)

  - Use a sliding window of embedding and classify the middle one

---
class: middle
# More Recent Word Embeddings

- FastText: Word embedding is a sum of substring embeddings (can be trained by
  backpropagation)

- Distillation of word embeddings from larger neural language models

- SoTA: finetuning with contextual embeddings (\*BERT\*), large language models

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Use TF-IDF for representing documents and explain its information-theoretical
  interpretation.

- Explain training of Word2Vec as a special case of logistic regression.

- Use pre-trained word embeddings for simple NLP tasks.

